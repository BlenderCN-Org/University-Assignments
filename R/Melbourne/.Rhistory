library(ISLR)
install.packages("ISLR")
library(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
library(leaps)
install.packages("leaps")
library(leaps)
regfit.full = regsubsets(Salary~., Hitters)
summary(regfit.full)
regfit.full = regsubsets(Salary~., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab = "RSS", type = "1")
plot(reg.summary$rss, xlab="Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq". type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab="Number of Variables", ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
coef(regfit.full, 6)
plot(regfit.full, scale="bic")
regfit.fwd = regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters), rep=TRUE)
test=(!train)
regfit.best=regsubsets(Salary~., data=Hitters[train,])
regfit.best=regsubsets(Salary~., data=Hitters[train,]. nvmax=19)
regfit.best=regsubsets(Salary~., data=Hitters[train,], nvmax=19)
test.mat=model.matrix(Salary~., data=Hitters[test,])
val.errors = rep(NA,19)
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
for(i in 1:19) {
coefi=coef(regfit.best,id='i')
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
for(i in 1:19) {
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
which.min(val.errosr)
which.min(val.errors
which.min(val.errors)
which.min(val.errors)
coef(regfit.best,10)
predict.regsubsets=function(object, newdata, id, ...) {
form=as.formula(object$call[[2]])
mat = model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%ceofi
}
regfit.best=regsubsets(Salary~., data=Hitters, nvmax=19)
coef(Regfit.best,10)
coef(regfit.best,10)
k=10
set.seed(1)
folds=sample(1:k, nrow(Hitters), replace=TRUE)
cv.errors=matrix(NA,k,19,dimnames=list(NULL,paste(1:19)))
for(j in 1:k) {
best.fit=regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
}
for(i in 1:19) {
pred=predict(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]~pred)^2)
}
predict.regsubsets=function(object, newdata, id, ...) {
form=as.formula(object$call[[2]])
mat = model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%ceofi
}
for(i in 1:19) {
pred=predict(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]~pred)^2)
}
predict.regsubsets=function(object, newdata, id, ...) {
form=as.formula(object$call[[2]])
mat = model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
for(i in 1:19) {
pred=predict(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]~pred)^2)
}
for(j in 1:k) {
best.fit=regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
for(i in 1:19) {
pred=predict(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]~pred)^2)
}
}
for(j in 1:k) {
best.fit=regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
for(i in 1:19) {
pred=predict(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
}
}
mean.cv.errors = apply(cv.error,2,mean)
library(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
library(leaps)
regfit.full = regsubsets(Salary~., Hitters)
summary(regfit.full)
regfit.full = regsubsets(Salary~., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab = "RSS", type = "l")""
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab="Number of Variables", ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
coef(regfit.full, 6)
regfit.fwd = regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters), rep=TRUE)
test=(!train)
regfit.best=regsubsets(Salary~., data=Hitters[train,], nvmax=19)
test.mat=model.matrix(Salary~., data=Hitters[test,])
val.errors = rep(NA,19)
for(i in 1:19) {
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best,10)
predict.regsubsets=function(object, newdata, id, ...) {
form=as.formula(object$call[[2]])
mat = model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
regfit.best=regsubsets(Salary~., data=Hitters, nvmax=19)
coef(regfit.best,10)
k=10
set.seed(1)
folds=sample(1:k, nrow(Hitters), replace=TRUE)
cv.errors=matrix(NA,k,19,dimnames=list(NULL,paste(1:19)))
for(j in 1:k) {
best.fit=regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
for(i in 1:19) {
pred=predict(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
}
}
mean.cv.errors = apply(cv.error,2,mean)
mean.cv.errors = apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors, type='b')
reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19)
coef(reg.best,11)
x=model.matrix(Salart~.,Hitters)[,1]
x=model.matrix(Salary~.,Hitters)[,1]
y=Hitters$Salary
library(gmnet)
install.packages(gmnet)
install.packages("gmnet")
library(gmnet)
install.packages('gmnet')
install.packages('glmnet')
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
# Run the previous lab first to get the environment state
x=model.matrix(Salary~.,Hitters)[,1]
y=Hitters$Salary
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
# Run the previous lab first to get the environment state
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
View(x)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[60]
coef(Ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod,s=50,type="coefficients")[1:20,]
set.seed(1)
train=sample(1:nrow(x), nrow(x/2))
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train,],alpha=0, lambda=grid,thresh=1e-12)
set.seed(1)
train=sample(1:nrow(x), nrow(x/2))
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train,],alpha=0, lambda=grid,thresh=1e-12)
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=3,newx=x[test,])
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred=y.test)^2)
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred=y.test)^2)
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred=y.test)^2)
mean((ridge.pred=y.test)^2)
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred=y.test)^2)
mean((ridge.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred=predict(ridge.mod, s=1e10, newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
predict(ridge.mod, s=0, exact=T, type="coefficients")[1:20,]
set.seed(1)
set.seed(1)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients"s=bestlam,newx=x[test,])
predict(out,type="coefficients",s=bestlam,newx=x[test,])
lasso.mod=glmnet(x[train,],y[train],alpha=1)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
library(pls)
set.seed(2)
install.packages("pls")
library(pls)
set.seed(2)
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
set.seed(1)
pcr.fit=pcr(Salary~., data=Hitters, subset=train,scale=TRUE, validation="CV")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
validationplot(pcr.fit, val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
?cv
?cv.glmnet
?cv.glmnet()
?"cv.glmnet"
?"cv.glmnet()"
?glmnet
?pcr
View(cv.out)
View(cv.errors)
install.packages("pls")
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
set.seed(1)
pcr.fit=pcr(Salary~., data=Hitters, subset=train,scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
library(naniar)
library(data.table)
library(glmnet)
library(readr)
library(Metrics)
library(tidyr)
library(fastDummies)
library(data.table)
library(glmnet)
library(readr)
library(Metrics)
library(tidyr)
library(fastDummies)
# Utility Setup ====
setwd("~/University-Assignments/R/Melbourne")
Mode <- function(x) {
ux <- na.omit(unique(x) )
tab <- tabulate(match(x, ux)); ux[tab == max(tab) ]
}
invlog.trans <- function(x) {
(10^(x))-1
}
# Loading Data ====
dt.train <- read_csv('train.csv')
dt.train <- data.frame(dt.train)
dt.test <- read_csv('test.csv')
dt.test$SalePrice = rep(0,length(dt.test$Id))
dt.test <- data.frame(dt.test)
#dt = rbindlist(list(dt.train, dt.test), use.names = TRUE)
dt = dt.train
#dt = dt.test
dt <- data.frame(dt)
# Feature Engineering
dt[,c("Id", "Alley", "PoolQC", "Fence", "MiscFeature", "FireplaceQu", "BsmtUnfSF", "MisFeature", "MiscVal")] <- NULL
dt$MasVnrType <- tidyr::replace_na(dt$MasVnrType, "NM")
dt$BsmtQual <- tidyr::replace_na(dt$BsmtQual, "NB")
dt$BsmtCond <- tidyr::replace_na(dt$BsmtCond, "NB")
dt$BsmtExposure <- tidyr::replace_na(dt$BsmtExposure, "NB")
dt$BsmtFinType1 <- tidyr::replace_na(dt$BsmtFinType1, "NB")
dt$BsmtFinType2 <- tidyr::replace_na(dt$BsmtFinType2, "NB")
dt[dt$BsmtQual == "NB", "BsmtFinSF1"] <- 0
dt[dt$BsmtQual == "NB", "BsmtFinSF2"] <- 0
dt[dt$BsmtQual == "NB", "BsmtHalfBath"] <- 0
dt[dt$BsmtQual == "NB", "BsmtFullBath"] <- 0
# Need to NOT HARDCODE these index values...
dt[949, "BsmtExposure"] <- Mode(dt$BsmtExposure)
dt[333, "BsmtFinType2"] <- Mode(dt$BsmtFinType2)
for(title in colnames(dt[, names(dt) != 'SalePrice'])) {
if(sum(is.na(dt[, title])) > 0) {
if(is.numeric(dt[, title])) {
dt[is.na(dt[,title]), title] <- mean(as.numeric(dt[,title]), na.rm=TRUE)
} else {
dt[is.na(dt[,title]), title] <- Mode(dt[,title])
}
}
}
remove(title)
dt[, "TotalSF"] = dt[, "TotalBsmtSF"] + dt[, "X1stFlrSF"] + dt[, "X2ndFlrSF"]
dt[dt$MasVnrArea == 0, "MasVnrType"] <- "NM"
dt[dt$MasVnrType == "NM", "MasVnrArea"] <- 0
# Note, can do more engineering here:
#   Such as comparing to other dates inside the dataset instead
#   of just imputing a mean value.
dt[dt$GarageYrBlt < 1950 | dt$GarageYrBlt > 2010, "GarageYrBlt"] <- mean(as.numeric(dt[dt$GarageYrBlt >= 1950 & dt$GarageYrBlt <= 2010, "GarageYrBlt"]), na.rm=TRUE)
dt[dt$YrSold < 1950 | dt$YrSold > 2010, "YrSold"] <- mean(as.numeric(dt[dt$YrSold >= 1950 & dt$YrSold <= 2010, "YrSold"]), na.rm=TRUE)
dt[dt$YearBuilt < 1950 | dt$YearBuilt > 2010, "YearBuilt"] <- mean(as.numeric(dt[dt$YearBuilt >= 1950 & dt$YearBuilt <= 2010, "YearBuilt"]), na.rm=TRUE)
dt[dt$YearRemodAdd < 1950 | dt$YearRemodAdd > 2010, "YearRemodAdd"] <- mean(as.numeric(dt[dt$YearRemodAdd >= 1950 & dt$YearRemodAdd <= 2010, "YearRemodAdd"]), na.rm=TRUE)
dt[dt$MoSold < 0 | dt$MoSold > 12, "MoSold"] <- mean(as.numeric(dt[dt$MoSold >= 0 & dt$MoSold <= 12, "MoSold"]), na.rm=TRUE)
col_numeric = c()
col_cat = c()
for(name in colnames(dt)) {
if(is.numeric(dt[1, name])) {
col_numeric <- c(col_numeric, name)
} else {
col_cat <- c(col_cat, name)
}
}
dt[,col_numeric] <- lapply(dt[,col_numeric], function(x) {log10(1+x)})
dt <- dummy_cols(dt)
for(name in colnames(dt)) {
if(!is.numeric(dt[1, name])) {
dt[,name] <- NULL
}
}
# Feature Selection ====
x = model.matrix(SalePrice~., dt)[,-1]
y = dt$SalePrice
set.seed(1)
#train = array(1:(length(dt.train$Id) + length(dt.test$Id)))
#train[1:length(dt.train$Id)] = TRUE
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]
# Cross validaiton Lasso Lambda
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam= cv.out$lambda.min
# Lasso Model Prediction  / Testing
lasso.mod = glmnet(x[train,], y[train], alpha=1)
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
rmsle(y.test, invlog.trans(lasso.pred))
View(dt.train)
# Loading Data ====
dt.train <- read_csv('train.csv')
dt.train <- data.frame(dt.train)
#dt = rbindlist(list(dt.train, dt.test), use.names = TRUE)
dt = dt.train
View(dt)
# Feature Engineering
dt[,c("Id", "Alley", "PoolQC", "Fence", "MiscFeature", "FireplaceQu", "BsmtUnfSF", "MiscVal")] <- NULL
